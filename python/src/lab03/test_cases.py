import sys
from src.lib.text import normalize, tokenize, count_freq, top_n

print("normalise:")
print(normalize("\r\n        word –ø—É—Ä—É–º–ø—É—Ä—É–º"))
print(normalize("Hello\r\nWorld"))
print(normalize("–ø—Ä–∏–≤–µ—Ç   –º–∏—Ä"))
print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞", yo2e = True))
print("tokenize:")
print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
print(tokenize("hello,world!!!"))
print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –Ω–µ –∫—Ä—É—Ç–æ"))
print(tokenize("1992 –≥–æ–¥"))
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))
print("count_freq:")
print(count_freq(["a","b","a","c","b","a"]))
print(count_freq(["bb","aa","bb","aa","cc"]))

freq = count_freq(["a","b","a","c","b","a"])
print(top_n(freq,2))
freq_1 = count_freq(["bb","aa","bb","aa","cc"])
print(top_n(freq_1, 2))
